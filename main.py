#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from src.utils import setup_logging, load_config
from src.crawler import BooksCrawler
import os
import sys
import logging
import platform
from pathlib import Path
from multiprocessing import Process
from src.utils import setup_logging, load_config
from src.crawler import BooksCrawler

def run_crawler(config, book_url, total_pages, delay):
    crawler = BooksCrawler(config)
    # å­é€²ç¨‹ä¸åšäººå·¥ CAPTCHA é©—è­‰
    crawler.login(auto_captcha=True)
    crawler.navigate_to_book(book_url)
    crawler.auto_capture_mode(total_pages, delay)

def print_banner():
    print("\n" + "="*70)
    print("ğŸ“š åšå®¢ä¾†é›»å­æ›¸æˆªåœ–å·¥å…· v2.0")
    print("="*70)
    print(f"ğŸ“Œ ç³»çµ±: {platform.system()}")
    print(f"ğŸ“Œ Python: {sys.version.split()[0]}")
    print("="*70 + "\n")
    # ...existing code...

def main():
    setup_logging()
    config = load_config()
    print_banner()
    # å…ˆç™»å…¥
    crawler = BooksCrawler(config)
    crawler.login(auto_captcha=False)

    # è®“ä½¿ç”¨è€…è¼¸å…¥å¤šæœ¬é›»å­æ›¸ç¶²å€
    urls_input = input("è«‹è¼¸å…¥æ‰€æœ‰é›»å­æ›¸ç¶²å€ï¼ˆä»¥é€—è™Ÿåˆ†éš”ï¼‰: ").strip()
    book_urls = [u.strip() for u in urls_input.split(",") if u.strip()]
    if not book_urls:
        print("æœªè¼¸å…¥ä»»ä½•ç¶²å€ï¼Œç¨‹å¼çµæŸã€‚")
        return
    total_pages = config.get("total_pages", 100)
    delay = config.get("delay", 1)
    processes = []
    # ä¸»é€²ç¨‹åŸ·è¡Œç¬¬ä¸€æœ¬æ›¸
    crawler.navigate_to_book(book_urls[0])
    crawler.auto_capture_mode(total_pages, delay)
    # å…¶é¤˜æ›¸ç±å¹³è¡Œè™•ç†
    for url in book_urls[1:]:
        p = Process(target=run_crawler, args=(config, url, total_pages, delay))
        p.start()
        processes.append(p)
    for p in processes:
        p.join()

if __name__ == "__main__":
    main()
