#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from src.utils import setup_logging, load_config
from src.crawler import BooksCrawler
import os
import sys
import logging
import platform
from pathlib import Path

# ç¢ºä¿ src åœ¨ Python è·¯å¾‘ä¸­
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

# å»ºç«‹å¿…è¦çš„è³‡æ–™å¤¾
for folder in ['src', 'output', 'logs', 'config']:
    Path(folder).mkdir(exist_ok=True)

# Import


def print_banner():
    """é¡¯ç¤ºç¨‹å¼æ©«å¹…"""
    print("\n" + "="*70)
    print("ğŸ“š åšå®¢ä¾†é›»å­æ›¸æˆªåœ–å·¥å…· v2.0")
    print("="*70)
    print(f"ğŸ“Œ ç³»çµ±: {platform.system()}")
    print(f"ğŸ“Œ Python: {sys.version.split()[0]}")
    print("="*70 + "\n")


def main():
    # é¡¯ç¤ºæ©«å¹…
    print_banner()

    # è¨­å®šæ—¥èªŒ
    setup_logging()
    logger = logging.getLogger(__name__)

    # è¼‰å…¥è¨­å®š
    config = load_config()

    crawler = None
    try:
        # å»ºç«‹çˆ¬èŸ²å¯¦ä¾‹
        print("ğŸš€ å•Ÿå‹•ç€è¦½å™¨...")
        crawler = BooksCrawler(config)

        # ç™»å…¥æµç¨‹
        print("\nğŸ“ ç™»å…¥æµç¨‹")
        print("-" * 40)
        if not crawler.ensure_login(auto_login=config.get('auto_login', False)):
            logger.error("ç™»å…¥å¤±æ•—ï¼Œç¨‹å¼ç„¡æ³•ç¹¼çºŒã€‚")
            return

        # å–å¾—æ›¸ç±ç¶²å€
        print("\nğŸ“– é›»å­æ›¸è¨­å®š")
        print("-" * 40)
        book_url = config.get('book_url')
        if not book_url:
            book_url = input("è«‹è¼¸å…¥é›»å­æ›¸ç¶²å€: ").strip()

        if not book_url:
            logger.error("æœªæä¾›é›»å­æ›¸ç¶²å€ï¼Œç¨‹å¼ç„¡æ³•ç¹¼çºŒã€‚")
            return

        # å°èˆªåˆ°æ›¸ç±é é¢
        crawler.navigate_to_book(book_url)

        # äº’å‹•å¼æç¤ºï¼Œè©¢å•ä½¿ç”¨è€…æ˜¯å¦é–‹å§‹æˆªåœ–
        print("\nğŸ“¸ æˆªåœ–æº–å‚™")
        print("-" * 40)
        user_input = input("å·²å®Œæˆå‰ç½®ä½œæ¥­ï¼Œæ˜¯å¦é–‹å§‹æˆªåœ–ï¼Ÿ (y/n): ").lower().strip()
        if user_input != 'y':
            logger.info("ä½¿ç”¨è€…å–æ¶ˆæ“ä½œï¼Œç¨‹å¼å³å°‡çµæŸã€‚")
            print("ä½¿ç”¨è€…å–æ¶ˆæ“ä½œï¼Œç¨‹å¼å³å°‡çµæŸã€‚")
            return

        # åŸ·è¡Œè‡ªå‹•æˆªåœ–æ¨¡å¼
        total_pages = config.get("total_pages", 100)
        delay = config.get("delay", 5)
        crawler.auto_capture_mode(total_pages, delay)

    except KeyboardInterrupt:
        logger.info("\nâš ï¸ ä½¿ç”¨è€…ä¸­æ–·ç¨‹å¼")
    except Exception as e:
        logger.error(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
        print(f"\nâŒ éŒ¯èª¤: {e}")
        print("è«‹æª¢æŸ¥éŒ¯èª¤æ—¥èªŒä»¥ç²å¾—æ›´å¤šè³‡è¨Š")
    finally:
        if crawler:
            print("\nğŸ”„ æ­£åœ¨é—œé–‰ç€è¦½å™¨...")
            crawler.close()
        print("âœ… ç¨‹å¼çµæŸ")


if __name__ == "__main__":
    main()
